{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to tweak for data creation\n",
    "np.random.seed(3215)\n",
    "sample_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data into environment\n",
    "# See name_data_explaination for data collection methods\n",
    "# AIAN - American Indian or Alaskan Native\n",
    "# API - Asian Pacific Islander\n",
    "last_names = pd.read_csv('data/common_surnames_census_2000.csv').rename(columns={'pct2prace': 'pctmixed'})\n",
    "first_names = pd.read_csv('data/ssa_names_db.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing for Last Names\n",
    "\n",
    "# Fields suppressed for confidentiality are assigned the value (S). \n",
    "# Replace confidentiality value with 0\n",
    "# Prevents conflicts when finding max(percentages)\n",
    "last_names2 = last_names.replace('(S)', 0.00)\n",
    "\n",
    "# Convert percentage columns from strings to floats\n",
    "for column in last_names2.columns[1:]:\n",
    "    if last_names2[column].dtype == 'object':\n",
    "        last_names2[column] = last_names2[column].astype(float)\n",
    "\n",
    "# Create new column based on the ethnicity label with highest probability\n",
    "last_names2['predominant'] = last_names2.iloc[:,5:].idxmax(1).str.replace('pct', '')\n",
    "\n",
    "# Sample evenly through each unique dominant ethnicity\n",
    "# Prevents most names being white and promotes even representation\n",
    "last_names_final = last_names2.groupby('predominant').apply(lambda ethnicity: ethnicity.sample(sample_size)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing for First Names\n",
    "# Multiply sample_size by 3 to keep same dimension as Last Names\n",
    "# 6 Ethnicities / 2 Genders\n",
    "first_names_final = first_names.groupby('gender').apply(lambda gender: gender.sample(sample_size*3)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Full Names dataset\n",
    "# Extract relevant features from First and Last Name datasets\n",
    "fnames = first_names_final.iloc[:,0]\n",
    "lnames = last_names_final.iloc[:,0].str.capitalize()\n",
    "ffeatures = first_names_final.iloc[:,1]\n",
    "lfeatures = last_names_final.iloc[:,5:]\n",
    "# Join all features together in final dataset\n",
    "full_names = pd.concat([fnames,lnames,lfeatures,ffeatures], axis= 1)\n",
    "full_names.columns = ['first', 'last', 'pctwhite', 'pctblack', 'pctapi', 'pctaian', 'pctmixed',\n",
    "       'pcthispanic', 'predominant', 'gender']\n",
    "# Make names into list for ChatGPT data collection\n",
    "names = [row for row in full_names[['first', 'last']].to_numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ChatGPT Response Generating Code\\n    Data saved in CSV file for future use'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''ChatGPT Response Generating Code\n",
    "    Data saved in CSV file for future use'''\n",
    "\n",
    "# import openai\n",
    "\n",
    "# openai.api_key = open('/Users/tuomasr/Library/Mobile Documents/com~apple~CloudDocs/School/ECS/ECS 171/Group Proj/key/Group_13_Project_Key.txt').read().strip('\\n')\n",
    "\n",
    "# reply_content = []\n",
    "# for person in names:\n",
    "#     name = ' '.join(person)\n",
    "#     text = f'Pretend you are a professor for at a popular university. You are asked by one of your students ({name}) if you can write them a letter of recommendation. Make up any information about them you feel is relevant to convey their abilities. Choose a field of study you believe is most fitting for them. Please do not include the heading'\n",
    "#     completion = openai.ChatCompletion.create(\n",
    "#         model=\"gpt-3.5-turbo\", # this is \"ChatGPT\" $0.002 per 1k tokens\n",
    "#         messages=[{\"role\": \"user\", \"content\": text}]\n",
    "#     )\n",
    "\n",
    "#     reply_content.append(completion.choices[0].message.content)\n",
    "\n",
    "# # pd.DataFrame(reply_content).to_csv('chatGPT_response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add letters of Rec to the database\n",
    "responses = pd.read_csv('chatGPT_responses')\n",
    "full_names['GPT_letters'] = responses.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminary functions needed for BERT modelling (Taken from article referenced in paper)\n",
    "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
    "    count = CountVectorizer(ngram_range=ngram_range, stop_words=\"english\").fit(documents)\n",
    "    t = count.transform(documents).toarray()\n",
    "    w = t.sum(axis=1)\n",
    "    tf = np.divide(t.T, w)\n",
    "    sum_t = t.sum(axis=0)\n",
    "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
    "    tf_idf = np.multiply(tf, idf)\n",
    "\n",
    "    return tf_idf, count\n",
    "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20):\n",
    "    words = count.get_feature_names_out()\n",
    "    labels = list(docs_per_topic.Topic)\n",
    "    tf_idf_transposed = tf_idf.T\n",
    "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
    "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
    "    return top_n_words\n",
    "\n",
    "def extract_topic_sizes(df):\n",
    "    topic_sizes = (df.groupby(['Topic'])\n",
    "                     .GPT_letters\n",
    "                     .count()\n",
    "                     .reset_index()\n",
    "                     .rename({\"Topic\": \"Topic\", 'GPT_letters': \"Size\"}, axis='columns')\n",
    "                     .sort_values(\"Size\", ascending=False))\n",
    "    return topic_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ethnicity: aian\n",
      "\n",
      "topic number: 1\n",
      "[('psychology', 0.004431030055974533), ('social', 0.004028192623017855), ('impressed', 0.003939407700139555), ('marketing', 0.003935389264733539), ('programming', 0.0039208006459563), ('intelligence', 0.00386168488882864), ('institution', 0.0038499604001698625), ('problem', 0.003696899348924165), ('talent', 0.0036746922015138176), ('course', 0.0036242550841283903), ('solving', 0.0035524954226970854), ('coding', 0.00354807640609547), ('critical', 0.0035158941468922717), ('thinking', 0.00347907408142779), ('study', 0.0034666787761228083)]\n",
      "topic number: 0\n",
      "[('sustainability', 0.006768981868051432), ('environment', 0.006504857943986859), ('scientific', 0.005900902898916807), ('conservation', 0.005775457670096796), ('kayda', 0.005477129146037217), ('issues', 0.004894762401784757), ('kingsleigh', 0.004806334242261191), ('calynn', 0.004806334242261191), ('knowledge', 0.004711224951480941), ('related', 0.004512654578700955), ('abijah', 0.004427883030097395), ('janiaya', 0.004427883030097395), ('wynter', 0.004427883030097395), ('jude', 0.004427883030097395), ('quincie', 0.004427883030097395)]\n",
      "topic number: -1\n",
      "[('enma', 0.02249607359698491), ('amberlyn', 0.015788651618769675), ('elannie', 0.015788651618769675), ('biology', 0.015285688460185392), ('jeniya', 0.014317611790127412), ('kiyana', 0.014317611790127412), ('codi', 0.012704547385725152), ('ruqayyah', 0.012704547385725152), ('annahi', 0.012704547385725152), ('sociology', 0.009022086639296956), ('journey', 0.008190523727400001), ('undergraduate', 0.007771761932774852), ('earned', 0.007158805895063706), ('scholarships', 0.006636171340845032), ('littledog', 0.006636171340845032)]\n",
      "\n",
      "Ethnicity: api\n",
      "\n",
      "topic number: 1\n",
      "[('psychology', 0.004469116837590283), ('contact', 0.003744033789227924), ('programming', 0.003705824208236165), ('hesitate', 0.0036429220959089082), ('study', 0.003605365871108371), ('information', 0.003605365871108371), ('subject', 0.003577159474404142), ('coding', 0.0035055778312818265), ('overall', 0.0033742104444928578), ('insightful', 0.0033742104444928578), ('intelligence', 0.003349833271171919), ('recommendation', 0.0032971106508828526), ('understanding', 0.003297109983610534), ('ethic', 0.0032898551833805364), ('institution', 0.003282249182706371)]\n",
      "topic number: 0\n",
      "[('sustainability', 0.008877911067896661), ('conservation', 0.007177557883998434), ('public', 0.006816252388010294), ('maeleigh', 0.0063534184924851494), ('issues', 0.006318727167242964), ('impact', 0.0061179477671488396), ('sirinity', 0.005923725932121385), ('julliana', 0.005923725932121385), ('health', 0.00582968204833695), ('local', 0.00577064219413283), ('climate', 0.0054572911927673546), ('winter', 0.0054572911927673546), ('tierra', 0.0054572911927673546), ('chrissie', 0.0054572911927673546), ('julianne', 0.0054572911927673546)]\n",
      "topic number: -1\n",
      "[('dinora', 0.015197412677325979), ('biology', 0.014868608248247482), ('dallys', 0.014169585662318403), ('estefany', 0.013053871149038882), ('sarah', 0.013053871149038882), ('nico', 0.013053871149038882), ('jesiah', 0.013053871149038882), ('besan', 0.013053871149038882), ('jaydenn', 0.011837632749340967), ('vidya', 0.011837632749340967), ('karizma', 0.011837632749340967), ('engineering', 0.011837632749340967), ('biomedical', 0.011837632749340967), ('biotechnology', 0.008403176082900395), ('capacity', 0.007891755166227312)]\n",
      "\n",
      "Ethnicity: black\n",
      "\n",
      "topic number: 1\n",
      "[('course', 0.0037553091081245914), ('insightful', 0.003614772099343), ('programming', 0.003614772099343), ('performance', 0.0035143692886443903), ('high', 0.0034944402332874446), ('addition', 0.0034842780924080148), ('health', 0.0034773595515320854), ('coding', 0.0034773595515320854), ('assignments', 0.003457608095023739), ('excellence', 0.0034260385443287624), ('qualities', 0.0033739516932025787), ('study', 0.0033416232929574206), ('communication', 0.0033176739524423586), ('mental', 0.0032995705144910157), ('believe', 0.003282489832735657)]\n",
      "topic number: 0\n",
      "[('sustainability', 0.010302435914818596), ('conservation', 0.009418162934479026), ('kaelee', 0.008931672199356267), ('blythe', 0.008931672199356267), ('lynley', 0.00840632679513842), ('issues', 0.007869829607230679), ('sustainable', 0.007837792534703663), ('shakira', 0.007837792534703663), ('coralie', 0.007837792534703663), ('graycelyn', 0.007837792534703663), ('jenan', 0.007220644010290708), ('kyrie', 0.007220644010290708), ('environment', 0.00710853942493526), ('azaleah', 0.006547891505259976), ('christyana', 0.006547891505259976)]\n",
      "topic number: -1\n",
      "[('biology', 0.013277625677122807), ('amiel', 0.011906793844705978), ('yoana', 0.010969251772851765), ('alaila', 0.010969251772851765), ('shaelyn', 0.010969251772851765), ('joselyne', 0.010969251772851765), ('bailee', 0.009947238833565243), ('camdynn', 0.009947238833565243), ('arihana', 0.009947238833565243), ('scientific', 0.008930095383529483), ('krisette', 0.008826553546122542), ('biochemistry', 0.008826553546122542), ('caoimhe', 0.008826553546122542), ('erla', 0.008826553546122542), ('analiya', 0.008826553546122542)]\n",
      "\n",
      "Ethnicity: hispanic\n",
      "\n",
      "topic number: -1\n",
      "[('thinking', 0.0028950447874182004), ('discussions', 0.002894423932568767), ('outstanding', 0.002894423932568767), ('asset', 0.002894423932568767), ('institution', 0.0028935384432159294), ('aptitude', 0.0028935384432159294), ('concepts', 0.0028935384432159294), ('contributions', 0.002891616774805518), ('believe', 0.002889960891987043), ('shown', 0.002889960891987043), ('communication', 0.002889960891987043), ('candidate', 0.002889960891987043), ('learning', 0.0028865608315609996), ('commitment', 0.0028865608315609996), ('teaching', 0.0028865608315609996)]\n",
      "\n",
      "Ethnicity: mixed\n",
      "\n",
      "topic number: -1\n",
      "[('curiosity', 0.0029284615597614293), ('commitment', 0.0029284615597614293), ('teaching', 0.0029284615597614293), ('shown', 0.0029278335385409427), ('asset', 0.0029278335385409427), ('impressive', 0.0029278335385409427), ('outstanding', 0.0029269378282076753), ('solving', 0.0029269378282076753), ('years', 0.0029269378282076753), ('courses', 0.0029269378282076753), ('various', 0.0029249939784631203), ('study', 0.0029249939784631203), ('achievements', 0.0029233189822066036), ('information', 0.0029233189822066036), ('understanding', 0.0029233189822066036)]\n",
      "\n",
      "Ethnicity: white\n",
      "\n",
      "topic number: 1\n",
      "[('development', 0.005226620765337122), ('technology', 0.004634059872540358), ('software', 0.004626964025200447), ('courses', 0.004353810139509233), ('master', 0.004234395543265945), ('aptitude', 0.004173024057718113), ('technical', 0.004081914913257589), ('coding', 0.003950240238636377), ('success', 0.0038818741610959114), ('believe', 0.003881135787237396), ('project', 0.00381044096192978), ('traeger', 0.003779475144378592), ('interpersonal', 0.003771754694820572), ('valuable', 0.0037123907530096836), ('data', 0.0037083156789645107)]\n",
      "topic number: -1\n",
      "[('psychology', 0.007199107951439034), ('environmental', 0.0071418181090023404), ('halsten', 0.006306499751970632), ('study', 0.006103371089152096), ('janiel', 0.005980740501283236), ('sani', 0.005628963760484895), ('cayse', 0.005248267312848324), ('rakim', 0.005248267312848324), ('deymar', 0.005248267312848324), ('personal', 0.005189514020509621), ('campus', 0.005159863433430517), ('savir', 0.004835018248968687), ('jess', 0.004835018248968687), ('pacer', 0.004835018248968687), ('kymere', 0.004835018248968687)]\n",
      "topic number: 0\n",
      "[('cormick', 0.0079294657234118), ('antonyo', 0.0079294657234118), ('past', 0.007417346828689299), ('caison', 0.007077567105769708), ('semesters', 0.0067678856423730935), ('raed', 0.006598899136011119), ('design', 0.006291170760684185), ('jaise', 0.006079301194816931), ('elijah', 0.006079301194816931), ('braylen', 0.006079301194816931), ('feynman', 0.006079301194816931), ('jaaziah', 0.00551288840645337), ('amori', 0.00551288840645337), ('chicago', 0.00551288840645337), ('mathematical', 0.00551288840645337)]\n"
     ]
    }
   ],
   "source": [
    "# Data grouped by ethnicity\n",
    "for ethnicity in full_names.predominant.unique():\n",
    "    data = full_names[full_names.predominant == ethnicity]['GPT_letters'].reset_index(drop=True)\n",
    "\n",
    "    model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "    embeddings = model.encode(data, show_progress_bar=False)\n",
    "\n",
    "    umap_embeddings = umap.UMAP(n_neighbors=15, \n",
    "                                n_components=5, \n",
    "                                metric='cosine').fit_transform(embeddings)\n",
    "\n",
    "    cluster = hdbscan.HDBSCAN(min_cluster_size=15,\n",
    "                            metric='euclidean',                      \n",
    "                            cluster_selection_method='eom').fit(umap_embeddings)\n",
    "\n",
    "    umap_data = umap.UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "    result = pd.DataFrame(umap_data, columns=['x', 'y'])\n",
    "    result['labels'] = cluster.labels_\n",
    "\n",
    "    docs_df = pd.DataFrame(data, columns=['GPT_letters'])\n",
    "    docs_df['Topic'] = cluster.labels_\n",
    "    docs_df['Doc_ID'] = range(len(docs_df))\n",
    "    docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'GPT_letters': ' '.join})\n",
    "\n",
    "    tf_idf, count = c_tf_idf(docs_per_topic.GPT_letters.values, m=len(data))\n",
    "\n",
    "    top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)\n",
    "    topic_sizes = extract_topic_sizes(docs_df); topic_sizes.head(10)\n",
    "    print()\n",
    "    print(f'Ethnicity: {ethnicity}')\n",
    "    print()\n",
    "    for topic_num in topic_sizes.iloc[:3,0].reset_index(drop=True):\n",
    "        print(f'topic number: {topic_num}')\n",
    "        print(top_n_words[topic_num][:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gender: F\n",
      "\n",
      "topic number: 0\n",
      "[('programming', 0.004091938961942296), ('coding', 0.003869526866097607), ('psychology', 0.003800121468495796), ('social', 0.0035733712777962423), ('course', 0.003443288580996198), ('study', 0.003299105464668051), ('teaching', 0.0032870723501859265), ('qualities', 0.003272619610638321), ('problem', 0.0032694763075480967), ('solving', 0.0032670331989879445), ('concepts', 0.003234567557282324), ('insightful', 0.003217871524342078), ('impressed', 0.003198373443188105), ('institution', 0.0031976097039796516), ('apart', 0.0031901273506359904)]\n",
      "topic number: 2\n",
      "[('sustainability', 0.009271690676707398), ('conservation', 0.0076547544412118316), ('issues', 0.006605032195035905), ('environment', 0.006534907050294234), ('local', 0.005284025756028724), ('sustainable', 0.005262891774977303), ('public', 0.004766302986805703), ('campus', 0.004762219987585508), ('impact', 0.004628585694482635), ('events', 0.0043013949128269825), ('health', 0.004250412551109281), ('world', 0.004231492839630412), ('climate', 0.004170428622157029), ('awareness', 0.0040933602694267915), ('initiatives', 0.004024357562817967)]\n",
      "topic number: 1\n",
      "[('biology', 0.024315244609394597), ('scientific', 0.013570357092964008), ('enma', 0.01194870586340575), ('biotechnology', 0.01064853857113796), ('dinora', 0.00924129460494285), ('calynn', 0.008490403356899245), ('sarah', 0.0077028129660925955), ('biomedical', 0.0077028129660925955), ('abijah', 0.0077028129660925955), ('biochemistry', 0.0077028129660925955), ('besan', 0.0077028129660925955), ('joselyne', 0.0077028129660925955), ('wynter', 0.0077028129660925955), ('shaelyn', 0.0077028129660925955), ('laboratory', 0.00742910293728684)]\n",
      "\n",
      "Gender: M\n",
      "\n",
      "topic number: 2\n",
      "[('communication', 0.0032370209094124552), ('valuable', 0.003179604092353818), ('coding', 0.0031654422557048667), ('technical', 0.003153428035274272), ('aptitude', 0.0031237709901666658), ('great', 0.003111640877853082), ('solving', 0.0030971902803383776), ('various', 0.003074708354360956), ('course', 0.0030624206473790767), ('doubt', 0.00306060202533712), ('problems', 0.0030556589186978077), ('knowledge', 0.00304478710886559), ('students', 0.0030394647545360183), ('study', 0.003032955702826803), ('believe', 0.0030280424293228953)]\n",
      "topic number: 0\n",
      "[('environmental', 0.03283007765077124), ('sustainability', 0.016491331352866023), ('conservation', 0.011655371545011582), ('kylar', 0.010901273659173575), ('issues', 0.01070120107802938), ('sustainable', 0.010595792313646893), ('impact', 0.010462670915520793), ('rylei', 0.010115070857653794), ('tabor', 0.010115070857653794), ('local', 0.009339338264243454), ('climate', 0.009293181879426578), ('pasha', 0.009293181879426578), ('change', 0.009293181879426578), ('romil', 0.009293181879426578), ('deymar', 0.009293181879426578)]\n",
      "topic number: 1\n",
      "[('win', 0.015062875915244267), ('jmari', 0.013976537234667775), ('pacer', 0.011649737058993009), ('jess', 0.011649737058993009), ('kymere', 0.011649737058993009), ('dylan', 0.011649737058993009), ('dusty', 0.011649737058993009), ('kerrigan', 0.011649737058993009), ('lindan', 0.011649737058993009), ('kealan', 0.010395100988737322), ('keland', 0.010395100988737322), ('javiel', 0.010395100988737322), ('mikhai', 0.010395100988737322), ('deakon', 0.010395100988737322), ('siddhartha', 0.009066307710854963)]\n"
     ]
    }
   ],
   "source": [
    "# Data to grouped by gender\n",
    "for gender in full_names.gender.unique():\n",
    "    data = full_names[full_names.gender == gender]['GPT_letters'].reset_index(drop=True)\n",
    "\n",
    "    model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "    embeddings = model.encode(data, show_progress_bar=False)\n",
    "\n",
    "    umap_embeddings = umap.UMAP(n_neighbors=15, \n",
    "                                n_components=5, \n",
    "                                metric='cosine').fit_transform(embeddings)\n",
    "\n",
    "    cluster = hdbscan.HDBSCAN(min_cluster_size=15,\n",
    "                            metric='euclidean',                      \n",
    "                            cluster_selection_method='eom').fit(umap_embeddings)\n",
    "\n",
    "    umap_data = umap.UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "    result = pd.DataFrame(umap_data, columns=['x', 'y'])\n",
    "    result['labels'] = cluster.labels_\n",
    "\n",
    "    docs_df = pd.DataFrame(data, columns=['GPT_letters'])\n",
    "    docs_df['Topic'] = cluster.labels_\n",
    "    docs_df['Doc_ID'] = range(len(docs_df))\n",
    "    docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'GPT_letters': ' '.join})\n",
    "\n",
    "    tf_idf, count = c_tf_idf(docs_per_topic.GPT_letters.values, m=len(data))\n",
    "\n",
    "    top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)\n",
    "    topic_sizes = extract_topic_sizes(docs_df); topic_sizes.head(10)\n",
    "    print()\n",
    "    print(f'Gender: {gender}')\n",
    "    print()\n",
    "    for topic_num in topic_sizes.iloc[:3,0].reset_index(drop=True):\n",
    "        print(f'topic number: {topic_num}')\n",
    "        print(top_n_words[topic_num][:15])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
