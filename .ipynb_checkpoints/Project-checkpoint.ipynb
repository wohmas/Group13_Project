{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to tweak for data creation\n",
    "np.random.seed(3215)\n",
    "sample_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data into environment\n",
    "# See name_data_explaination for data collection methods\n",
    "# AIAN - American Indian or Alaskan Native\n",
    "# API - Asian Pacific Islander\n",
    "last_names = pd.read_csv('data/common_surnames_census_2000.csv').rename(columns={'pct2prace': 'pctmixed'})\n",
    "first_names = pd.read_csv('data/ssa_names_db.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing for Last Names\n",
    "\n",
    "# Fields suppressed for confidentiality are assigned the value (S). \n",
    "# Replace confidentiality value with 0\n",
    "# Prevents conflicts when finding max(percentages)\n",
    "last_names2 = last_names.replace('(S)', 0.00)\n",
    "\n",
    "# Convert percentage columns from strings to floats\n",
    "for column in last_names2.columns[1:]:\n",
    "    if last_names2[column].dtype == 'object':\n",
    "        last_names2[column] = last_names2[column].astype(float)\n",
    "\n",
    "# Create new column based on the ethnicity label with highest probability\n",
    "last_names2['predominant'] = last_names2.iloc[:,5:].idxmax(1).str.replace('pct', '')\n",
    "\n",
    "# Sample evenly through each unique dominant ethnicity\n",
    "# Prevents most names being white and promotes even representation\n",
    "last_names_final = last_names2.groupby('predominant').apply(lambda ethnicity: ethnicity.sample(sample_size)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nt = last_names.replace('(S)', None)\\nt.dropna()\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Potential other way of handling confidentiality\n",
    "# 27,649 Names after removal\n",
    "'''\n",
    "t = last_names.replace('(S)', None)\n",
    "t.dropna()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing for First Names\n",
    "# Multiply sample_size by 3 to keep same dimension as Last Names\n",
    "# 6 Ethnicities / 2 Genders\n",
    "first_names_final = first_names.groupby('gender').apply(lambda gender: gender.sample(sample_size*3)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Full Names dataset\n",
    "# Extract relevant features from First and Last Name datasets\n",
    "fnames = first_names_final.iloc[:,0]\n",
    "lnames = last_names_final.iloc[:,0].str.capitalize()\n",
    "ffeatures = first_names_final.iloc[:,1]\n",
    "lfeatures = last_names_final.iloc[:,5:]\n",
    "# Join all features together in final dataset\n",
    "full_names = pd.concat([fnames,lnames,lfeatures,ffeatures], axis= 1)\n",
    "full_names.columns = ['first', 'last', 'pctwhite', 'pctblack', 'pctapi', 'pctaian', 'pctmixed',\n",
    "       'pcthispanic', 'predominant', 'gender']\n",
    "# Make names into list for ChatGPT data collection\n",
    "names = [row for row in full_names[['first', 'last']].to_numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ChatGPT Response Generating Code\n",
    "    Data saved in CSV file for future use'''\n",
    "\n",
    "# import openai\n",
    "\n",
    "# openai.api_key = open('/Users/tuomasr/Library/Mobile Documents/com~apple~CloudDocs/School/ECS/ECS 171/Group Proj/key/Group_13_Project_Key.txt').read().strip('\\n')\n",
    "\n",
    "# reply_content = []\n",
    "# for person in names:\n",
    "#     name = ' '.join(person)\n",
    "#     text = f'Pretend you are a professor for at a popular university. You are asked by one of your students ({name}) if you can write them a letter of recommendation. Make up any information about them you feel is relevant to convey their abilities. Choose a field of study you believe is most fitting for them. Please do not include the heading'\n",
    "#     completion = openai.ChatCompletion.create(\n",
    "#         model=\"gpt-3.5-turbo\", # this is \"ChatGPT\" $0.002 per 1k tokens\n",
    "#         messages=[{\"role\": \"user\", \"content\": text}]\n",
    "#     )\n",
    "\n",
    "#     reply_content.append(completion.choices[0].message.content)\n",
    "\n",
    "# # pd.DataFrame(reply_content).to_csv('chatGPT_response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add letters of Rec to the database\n",
    "responses = pd.read_csv('chatGPT_responses')\n",
    "full_names['GPT_letters'] = responses.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Full_Name</th>\n",
       "      <th>10</th>\n",
       "      <th>101</th>\n",
       "      <th>19</th>\n",
       "      <th>2020</th>\n",
       "      <th>2021</th>\n",
       "      <th>30</th>\n",
       "      <th>81</th>\n",
       "      <th>85</th>\n",
       "      <th>_______</th>\n",
       "      <th>...</th>\n",
       "      <th>ziff</th>\n",
       "      <th>zin</th>\n",
       "      <th>ziyu</th>\n",
       "      <th>zohaib</th>\n",
       "      <th>zohra</th>\n",
       "      <th>zola</th>\n",
       "      <th>zone</th>\n",
       "      <th>zuni</th>\n",
       "      <th>zuriana</th>\n",
       "      <th>zyanna</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baeley Parisien</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mahniya Charley</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Athziry Greyeyes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crystel Whitebird</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vionna Dumarce</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>Avishai Chenowith</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>Carmyne Crisp</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>Dream Cloran</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>Aviel Sago</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>Deymar Burkhard</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows Ã— 4685 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Full_Name   10  101   19  2020      2021   30   81   85  _______  \\\n",
       "0      Baeley Parisien  0.0  0.0  0.0   0.0  0.000000  0.0  0.0  0.0      0.0   \n",
       "1      Mahniya Charley  0.0  0.0  0.0   0.0  0.000000  0.0  0.0  0.0      0.0   \n",
       "2     Athziry Greyeyes  0.0  0.0  0.0   0.0  0.000000  0.0  0.0  0.0      0.0   \n",
       "3    Crystel Whitebird  0.0  0.0  0.0   0.0  0.081326  0.0  0.0  0.0      0.0   \n",
       "4       Vionna Dumarce  0.0  0.0  0.0   0.0  0.000000  0.0  0.0  0.0      0.0   \n",
       "..                 ...  ...  ...  ...   ...       ...  ...  ...  ...      ...   \n",
       "595  Avishai Chenowith  0.0  0.0  0.0   0.0  0.000000  0.0  0.0  0.0      0.0   \n",
       "596      Carmyne Crisp  0.0  0.0  0.0   0.0  0.000000  0.0  0.0  0.0      0.0   \n",
       "597       Dream Cloran  0.0  0.0  0.0   0.0  0.000000  0.0  0.0  0.0      0.0   \n",
       "598         Aviel Sago  0.0  0.0  0.0   0.0  0.000000  0.0  0.0  0.0      0.0   \n",
       "599    Deymar Burkhard  0.0  0.0  0.0   0.0  0.000000  0.0  0.0  0.0      0.0   \n",
       "\n",
       "     ...  ziff  zin  ziyu  zohaib  zohra  zola  zone  zuni  zuriana  zyanna  \n",
       "0    ...   0.0  0.0   0.0     0.0    0.0   0.0   0.0   0.0      0.0     0.0  \n",
       "1    ...   0.0  0.0   0.0     0.0    0.0   0.0   0.0   0.0      0.0     0.0  \n",
       "2    ...   0.0  0.0   0.0     0.0    0.0   0.0   0.0   0.0      0.0     0.0  \n",
       "3    ...   0.0  0.0   0.0     0.0    0.0   0.0   0.0   0.0      0.0     0.0  \n",
       "4    ...   0.0  0.0   0.0     0.0    0.0   0.0   0.0   0.0      0.0     0.0  \n",
       "..   ...   ...  ...   ...     ...    ...   ...   ...   ...      ...     ...  \n",
       "595  ...   0.0  0.0   0.0     0.0    0.0   0.0   0.0   0.0      0.0     0.0  \n",
       "596  ...   0.0  0.0   0.0     0.0    0.0   0.0   0.0   0.0      0.0     0.0  \n",
       "597  ...   0.0  0.0   0.0     0.0    0.0   0.0   0.0   0.0      0.0     0.0  \n",
       "598  ...   0.0  0.0   0.0     0.0    0.0   0.0   0.0   0.0      0.0     0.0  \n",
       "599  ...   0.0  0.0   0.0     0.0    0.0   0.0   0.0   0.0      0.0     0.0  \n",
       "\n",
       "[600 rows x 4685 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = full_names['GPT_letters'].copy()\n",
    "\n",
    "# process data with tf-idf \n",
    "vectorizer = TfidfVectorizer()\n",
    "scores = vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "# get term/feature names\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# form table of names with tf-idf scores corresponding to each term in their responses\n",
    "names = full_names['first'] + ' ' + full_names['last']\n",
    "name_scores = pd.DataFrame(scores, columns=terms)\n",
    "name_scores.insert(0, 'Full_Name', names)\n",
    "\n",
    "# put into csv file?\n",
    "display(name_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
