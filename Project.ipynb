{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\socce\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/socce/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to tweak for data creation\n",
    "np.random.seed(3215)\n",
    "sample_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data into environment\n",
    "# See name_data_explaination for data collection methods\n",
    "# AIAN - American Indian or Alaskan Native\n",
    "# API - Asian Pacific Islander\n",
    "last_names = pd.read_csv('data/common_surnames_census_2000.csv').rename(columns={'pct2prace': 'pctmixed'})\n",
    "first_names = pd.read_csv('data/ssa_names_db.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing for Last Names\n",
    "\n",
    "# Fields suppressed for confidentiality are assigned the value (S). \n",
    "# Replace confidentiality value with 0\n",
    "# Prevents conflicts when finding max(percentages)\n",
    "last_names2 = last_names.replace('(S)', 0.00)\n",
    "\n",
    "# Convert percentage columns from strings to floats\n",
    "for column in last_names2.columns[1:]:\n",
    "    if last_names2[column].dtype == 'object':\n",
    "        last_names2[column] = last_names2[column].astype(float)\n",
    "\n",
    "# Create new column based on the ethnicity label with highest probability\n",
    "last_names2['predominant'] = last_names2.iloc[:,5:].idxmax(1).str.replace('pct', '')\n",
    "\n",
    "# Sample evenly through each unique dominant ethnicity\n",
    "# Prevents most names being white and promotes even representation\n",
    "last_names_final = last_names2.groupby('predominant').apply(lambda ethnicity: ethnicity.sample(sample_size)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nt = last_names.replace('(S)', None)\\nt.dropna()\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Potential other way of handling confidentiality\n",
    "# 27,649 Names after removal\n",
    "'''\n",
    "t = last_names.replace('(S)', None)\n",
    "t.dropna()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing for First Names\n",
    "# Multiply sample_size by 3 to keep same dimension as Last Names\n",
    "# 6 Ethnicities / 2 Genders\n",
    "first_names_final = first_names.groupby('gender').apply(lambda gender: gender.sample(sample_size*3)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Full Names dataset\n",
    "# Extract relevant features from First and Last Name datasets\n",
    "fnames = first_names_final.iloc[:,0]\n",
    "lnames = last_names_final.iloc[:,0].str.capitalize()\n",
    "ffeatures = first_names_final.iloc[:,1]\n",
    "lfeatures = last_names_final.iloc[:,5:]\n",
    "# Join all features together in final dataset\n",
    "full_names = pd.concat([fnames,lnames,lfeatures,ffeatures], axis= 1)\n",
    "full_names.columns = ['first', 'last', 'pctwhite', 'pctblack', 'pctapi', 'pctaian', 'pctmixed',\n",
    "       'pcthispanic', 'predominant', 'gender']\n",
    "# Make names into list for ChatGPT data collection\n",
    "names = [row for row in full_names[['first', 'last']].to_numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ChatGPT Response Generating Code\n",
    "    Data saved in CSV file for future use'''\n",
    "\n",
    "# import openai\n",
    "\n",
    "# openai.api_key = open('/Users/tuomasr/Library/Mobile Documents/com~apple~CloudDocs/School/ECS/ECS 171/Group Proj/key/Group_13_Project_Key.txt').read().strip('\\n')\n",
    "\n",
    "# reply_content = []\n",
    "# for person in names:\n",
    "#     name = ' '.join(person)\n",
    "#     text = f'Pretend you are a professor for at a popular university. You are asked by one of your students ({name}) if you can write them a letter of recommendation. Make up any information about them you feel is relevant to convey their abilities. Choose a field of study you believe is most fitting for them. Please do not include the heading'\n",
    "#     completion = openai.ChatCompletion.create(\n",
    "#         model=\"gpt-3.5-turbo\", # this is \"ChatGPT\" $0.002 per 1k tokens\n",
    "#         messages=[{\"role\": \"user\", \"content\": text}]\n",
    "#     )\n",
    "\n",
    "#     reply_content.append(completion.choices[0].message.content)\n",
    "\n",
    "# # pd.DataFrame(reply_content).to_csv('chatGPT_response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       first       last  pctwhite  pctblack  pctapi  pctaian  pctmixed  \\\n",
      "0     Baeley   Parisien     36.57      8.43    0.00    47.22      6.20   \n",
      "1    Mahniya    Charley     19.43     16.29    1.83    59.07      2.16   \n",
      "2    Athziry   Greyeyes      0.00      0.00    0.00    91.40      4.07   \n",
      "3    Crystel  Whitebird     17.18      0.00    0.00    72.90      7.63   \n",
      "4     Vionna    Dumarce      5.59      0.00    0.00    84.92      3.35   \n",
      "..       ...        ...       ...       ...     ...      ...       ...   \n",
      "595  Avishai  Chenowith     96.97      0.00    0.00     0.00      0.00   \n",
      "596  Carmyne      Crisp     83.79     12.07    0.37     0.59      1.70   \n",
      "597    Dream     Cloran     99.63      0.00    0.00     0.00      0.00   \n",
      "598    Aviel       Sago     46.92     39.74    1.41     6.41      1.92   \n",
      "599   Deymar   Burkhard     95.39      0.46    1.00     0.00      1.46   \n",
      "\n",
      "     pcthispanic predominant gender  \\\n",
      "0           0.00        aian      F   \n",
      "1           1.23        aian      F   \n",
      "2           3.17        aian      F   \n",
      "3           1.91        aian      F   \n",
      "4           4.47        aian      F   \n",
      "..           ...         ...    ...   \n",
      "595         0.00       white      M   \n",
      "596         1.48       white      M   \n",
      "597         0.00       white      M   \n",
      "598         3.59       white      M   \n",
      "599         1.69       white      M   \n",
      "\n",
      "                                           GPT_letters  \n",
      "0    Dear Admissions Committee,\\n\\nIt is my pleasur...  \n",
      "1    Dear Mr./Ms. Charley,\\n\\nI am thrilled to writ...  \n",
      "2    and greeting of the letter in your response.\\n...  \n",
      "3    November 1, 2021\\n\\nDear Admission Committee,\\...  \n",
      "4    Dear Admissions Committee,\\n\\nI am writing thi...  \n",
      "..                                                 ...  \n",
      "595  To Whom It May Concern\\n\\nI am writing this le...  \n",
      "596  Dear Admissions Committee,\\n\\nI am writing thi...  \n",
      "597  Dear Admissions Committee,\\n\\nI am writing to ...  \n",
      "598  and date of the letter.\\n\\nDear Admissions Com...  \n",
      "599  To Whom It May Concern,\\n\\nI am writing this l...  \n",
      "\n",
      "[600 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "# Add letters of Rec to the database\n",
    "responses = pd.read_csv('chatGPT_responses')\n",
    "full_names['GPT_letters'] = responses.iloc[:,1]\n",
    "print(full_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Full_Name</th>\n",
       "      <th>Pre_Race</th>\n",
       "      <th>Pre_Gender</th>\n",
       "      <th>10</th>\n",
       "      <th>101</th>\n",
       "      <th>19</th>\n",
       "      <th>2020</th>\n",
       "      <th>2021</th>\n",
       "      <th>30</th>\n",
       "      <th>81</th>\n",
       "      <th>...</th>\n",
       "      <th>ziff</th>\n",
       "      <th>zin</th>\n",
       "      <th>ziyu</th>\n",
       "      <th>zohaib</th>\n",
       "      <th>zohra</th>\n",
       "      <th>zola</th>\n",
       "      <th>zone</th>\n",
       "      <th>zuni</th>\n",
       "      <th>zuriana</th>\n",
       "      <th>zyanna</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baeley Parisien</td>\n",
       "      <td>aian</td>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mahniya Charley</td>\n",
       "      <td>aian</td>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Athziry Greyeyes</td>\n",
       "      <td>aian</td>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crystel Whitebird</td>\n",
       "      <td>aian</td>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vionna Dumarce</td>\n",
       "      <td>aian</td>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>Avishai Chenowith</td>\n",
       "      <td>white</td>\n",
       "      <td>M</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>Carmyne Crisp</td>\n",
       "      <td>white</td>\n",
       "      <td>M</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>Dream Cloran</td>\n",
       "      <td>white</td>\n",
       "      <td>M</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>Aviel Sago</td>\n",
       "      <td>white</td>\n",
       "      <td>M</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>Deymar Burkhard</td>\n",
       "      <td>white</td>\n",
       "      <td>M</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 4687 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Full_Name Pre_Race Pre_Gender   10  101   19  2020      2021  \\\n",
       "0      Baeley Parisien     aian          F  0.0  0.0  0.0   0.0  0.000000   \n",
       "1      Mahniya Charley     aian          F  0.0  0.0  0.0   0.0  0.000000   \n",
       "2     Athziry Greyeyes     aian          F  0.0  0.0  0.0   0.0  0.000000   \n",
       "3    Crystel Whitebird     aian          F  0.0  0.0  0.0   0.0  0.081326   \n",
       "4       Vionna Dumarce     aian          F  0.0  0.0  0.0   0.0  0.000000   \n",
       "..                 ...      ...        ...  ...  ...  ...   ...       ...   \n",
       "595  Avishai Chenowith    white          M  0.0  0.0  0.0   0.0  0.000000   \n",
       "596      Carmyne Crisp    white          M  0.0  0.0  0.0   0.0  0.000000   \n",
       "597       Dream Cloran    white          M  0.0  0.0  0.0   0.0  0.000000   \n",
       "598         Aviel Sago    white          M  0.0  0.0  0.0   0.0  0.000000   \n",
       "599    Deymar Burkhard    white          M  0.0  0.0  0.0   0.0  0.000000   \n",
       "\n",
       "      30   81  ...  ziff  zin  ziyu  zohaib  zohra  zola  zone  zuni  zuriana  \\\n",
       "0    0.0  0.0  ...   0.0  0.0   0.0     0.0    0.0   0.0   0.0   0.0      0.0   \n",
       "1    0.0  0.0  ...   0.0  0.0   0.0     0.0    0.0   0.0   0.0   0.0      0.0   \n",
       "2    0.0  0.0  ...   0.0  0.0   0.0     0.0    0.0   0.0   0.0   0.0      0.0   \n",
       "3    0.0  0.0  ...   0.0  0.0   0.0     0.0    0.0   0.0   0.0   0.0      0.0   \n",
       "4    0.0  0.0  ...   0.0  0.0   0.0     0.0    0.0   0.0   0.0   0.0      0.0   \n",
       "..   ...  ...  ...   ...  ...   ...     ...    ...   ...   ...   ...      ...   \n",
       "595  0.0  0.0  ...   0.0  0.0   0.0     0.0    0.0   0.0   0.0   0.0      0.0   \n",
       "596  0.0  0.0  ...   0.0  0.0   0.0     0.0    0.0   0.0   0.0   0.0      0.0   \n",
       "597  0.0  0.0  ...   0.0  0.0   0.0     0.0    0.0   0.0   0.0   0.0      0.0   \n",
       "598  0.0  0.0  ...   0.0  0.0   0.0     0.0    0.0   0.0   0.0   0.0      0.0   \n",
       "599  0.0  0.0  ...   0.0  0.0   0.0     0.0    0.0   0.0   0.0   0.0      0.0   \n",
       "\n",
       "     zyanna  \n",
       "0       0.0  \n",
       "1       0.0  \n",
       "2       0.0  \n",
       "3       0.0  \n",
       "4       0.0  \n",
       "..      ...  \n",
       "595     0.0  \n",
       "596     0.0  \n",
       "597     0.0  \n",
       "598     0.0  \n",
       "599     0.0  \n",
       "\n",
       "[600 rows x 4687 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = full_names['GPT_letters'].copy()\n",
    "\n",
    "# process data with tf-idf \n",
    "vectorizer = TfidfVectorizer()\n",
    "scores = vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "# get term/feature names\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# form table of names with tf-idf scores corresponding to each term in their responses\n",
    "names = full_names['first'] + ' ' + full_names['last']\n",
    "name_scores = pd.DataFrame(scores, columns=terms)\n",
    "name_scores.insert(0, 'Full_Name', names)\n",
    "name_scores.insert(1, 'Pre_Race', full_names['predominant'])\n",
    "name_scores.insert(2, 'Pre_Gender', full_names['gender'])\n",
    "\n",
    "# put into csv file?\n",
    "display(name_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copied = name_scores.copy()\n",
    "data_copied = data_copied.drop(columns = [\"Full_Name\", \"Pre_Race\", \"Pre_Gender\"])\n",
    "data_word_clouds = data_copied.transpose()\n",
    "\n",
    "\n",
    "# change the value to black\n",
    "def black_color (word, font_size, position, orientation, random_state = None, **kwargs):\n",
    "    return (\"hsl(0, 100%, 1%)\")\n",
    "\n",
    "# set the background color to white\n",
    "# word cloud for the fisrt person \"Baeley Parisien\"\n",
    "wordcloud = WordCloud(background_color = \"white\", width = 900, height = 500, max_words = 500).generate_from_frequencies(data_word_clouds[0])\n",
    "\n",
    "# set the word color to black\n",
    "wordcloud.recolor(color_func = black_color)\n",
    "\n",
    "# set the figsize\n",
    "# plt.figure(figsize = [15, 10])\n",
    "       \n",
    "plt.imshow(wordcloud, interpolation = \"bilinear\")\n",
    "\n",
    "plt.axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
